# -*- coding: utf-8 -*-
"""Losses.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nkVycHHUOQtUFB9I0EePMfyWymMVeEfQ
"""

import numpy as np
import sys
sys.path.append("C:/Users/hp/Documents/DOUTORADO INF/AULAS DOUTORADO/PROJETO FINAL DE PROGRAMACAO\ENTREGA_FINAL INF2102/modulos/processing_data.py")
from processing_data import adjust_labels
'''
Opções personalizadas de perdas que podem ser usadas na arquitetura de rede neural, porém por defeito
a perda usada é 'binary_crossentropy'. Caso requira-se ser mudada siga as instruções do documento anexo 
na secção de guia de usuario: informações para desenvolvedores
'''

def weight_class():

  #Fonte: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights
    pos, neg, null, total = 0,0,0,0
    
    for i in range(0, np.size(tr_files, 0)):
    
        record_name = tr_files.header.values[i][:-4]
        arousals = list(adjust_labels(record_name, FreqSample))
        
        p = [n for n in arousals if n  == 1]
        n = [n for n in arousals if n  == 0]
        m = [n for n in arousals if n  == -1]
        
        total += len(arousals)
        pos += len(p)
        neg += len(n)
        null += len(m)
    
    w_0 = (1/neg)*(total)/3.0
    w_1 = (1/pos)*(total)/3.0
    w_m1 = (1/null)*(total)/3.0
    
    print('Weight for class 0: {:.2f}'.format(w_0))
    print('Weight for class 1: {:.2f}'.format(w_1))
    print('Weight for class -1: {:.2f}'.format(w_m1))
    class_weight = {0: w_0, 1: w_1, -1:w_m1}
    
    return class_weight 

def focal_loss(gamma=2., alpha=.25):
    from keras import backend as k
    import tensorflow as tf
    # https://arxiv.org/pdf/1708.02002.pdf
    # https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/

    gamma = float(gamma)
    alpha = float(alpha)

    def focal_loss_fixed(y_true, y_pred):
        """Focal loss for multi-classification
        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)
        Notice: y_pred is probability after softmax
        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper
        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)
        Focal Loss for Dense Object Detection
        https://arxiv.org/abs/1708.02002

        Arguments:
            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]
            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]

        Keyword Arguments:
            gamma {float} -- (default: {2.0})
            alpha {float} -- (default: {0.25})

        Returns:
            [tensor] -- loss.
        """
        epsilon = k.epsilon()
        y_true = tf.convert_to_tensor(y_true, tf.float32)
        y_pred = tf.convert_to_tensor(y_pred, tf.float32)

        model_out = tf.add(y_pred, epsilon)
        ce = tf.multiply(y_true, -tf.math.log(model_out))
        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))
        fl = tf.multiply(alpha, tf.multiply(weight, ce))
        reduced_fl = tf.reduce_max(fl, axis=1)
        return tf.reduce_mean(reduced_fl)
    
    return focal_loss_fixed

def categorical_focal_loss(alpha, gamma=2.):
    
    from keras import backend as K

    """
    Softmax version of focal loss.
    When there is a skew between different categories/labels in your data set, you can try to apply this function as a
    loss.
          m
      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)
          c=1
      where m = number of classes, c = class and o = observation
    Parameters:
      alpha -- the same as weighing factor in balanced cross entropy. Alpha is used to specify the weight of different
      categories/labels, the size of the array needs to be consistent with the number of classes.
      gamma -- focusing parameter for modulating factor (1-p)
    Default value:
      gamma -- 2.0 as mentioned in the paper
      alpha -- 0.25 as mentioned in the paper
    References:
        Official paper: https://arxiv.org/pdf/1708.02002.pdf
        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy
    Usage:
    model.compile(loss=[categorical_focal_loss(alpha=[[.25, .25, .25]], gamma=2)], metrics=["accuracy"], optimizer=adam)
    """

    alpha = np.array(alpha, dtype=np.float32)

    def categorical_focal_loss_fixed(y_true, y_pred):
        """
        :param y_true: A tensor of the same shape as `y_pred`
        :param y_pred: A tensor resulting from a softmax
        :return: Output tensor.
        """

        # Scale predictions so that the class probas of each sample sum to 1
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)

        # Clip the prediction value to prevent NaN's and Inf's
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)

        # Calculate Cross Entropy
        cross_entropy = -y_true * K.log(y_pred)

        # Calculate Focal Loss
        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy

        # Compute mean loss in mini_batch
        return K.mean(K.sum(loss, axis=-1))

    return categorical_focal_loss_fixed